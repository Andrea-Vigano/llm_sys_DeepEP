[config] num_tokens=4096, hidden=7168, num_topk=8
[layout] Kernel performance: 0.036 ms

[testing] Running with BF16, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed

[tuning] SMs 24, NVL chunk 4: 125.85 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 8: 174.24 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 12: 193.02 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 16: 210.72 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 20: 212.78 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 24: 223.54 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 28: 221.43 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 32: 232.48 GB/s (NVL) 
[tuning] Best dispatch (FP8): SMs 24, NVL chunk 32, 232.48 GB/s (NVL)

[tuning] SMs 24, NVL chunk 4: 190.83 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 8: 271.41 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 12: 279.73 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 16: 308.61 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 20: 294.46 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 24: 316.28 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 28: 301.29 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 32: 317.01 GB/s (NVL) 
[tuning] Best dispatch (BF16): SMs 24, NVL chunk 32, 317.01 GB/s (NVL)

[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
[tuning] SMs 24, NVL chunk 1: 51.84 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 2: 99.26 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 3: 143.09 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 4: 182.69 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 5: 215.36 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 6: 246.37 GB/s (NVL) 
[tuning] Best combine: SMs 24, NVL chunk 6: 246.37 GB/s (NVL)


[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[rank0]:[W420 20:36:11.068123617 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
