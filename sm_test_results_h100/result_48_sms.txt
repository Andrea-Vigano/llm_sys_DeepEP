[config] num_tokens=4096, hidden=7168, num_topk=8
[layout] Kernel performance: 0.036 ms

[testing] Running with BF16, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=False, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=True, previous=False) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=False, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with BF16, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, with top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed
[testing] Running with FP8, without top-k (async=True, previous=True) ...
[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
 passed

[tuning] SMs 24, NVL chunk 4: 124.02 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 8: 173.66 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 12: 193.33 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 16: 210.77 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 20: 213.02 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 24: 223.87 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 28: 220.66 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 32: 232.55 GB/s (NVL) 
[tuning] Best dispatch (FP8): SMs 24, NVL chunk 32, 232.55 GB/s (NVL)

[tuning] SMs 24, NVL chunk 4: 190.96 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 8: 270.88 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 12: 280.14 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 16: 308.83 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 20: 294.44 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 24: 315.22 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 28: 300.86 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 32: 317.47 GB/s (NVL) 
[tuning] Best dispatch (BF16): SMs 24, NVL chunk 32, 317.47 GB/s (NVL)

[0] Dispatching tensor([4083, 4083], device='cuda:0', dtype=torch.int32) tokens to 2 ranks
[tuning] SMs 24, NVL chunk 1: 51.55 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 2: 98.73 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 3: 142.55 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 4: 182.05 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 5: 214.73 GB/s (NVL) 
[tuning] SMs 24, NVL chunk 6: 246.48 GB/s (NVL) 
[tuning] Best combine: SMs 24, NVL chunk 6: 246.48 GB/s (NVL)


[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[1] Dispatching tensor([4079, 4086], device='cuda:1', dtype=torch.int32) tokens to 2 ranks
[rank0]:[W420 20:36:39.131382094 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
